---
title: "Clustering Airlines Customers"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

East-West Airlines is trying to learn more about its customers. Key issues are their flying patterns, earning and use of frequent flyer rewards, and use of the airline credit card. The task is to identify customer segments via clustering. The file *EastWestAirlines.xlsx* contains information on 4000 passengers who belong to an airline's frequent flier program. For each passenger the data include information on their mileage history and on different ways they accrued or spent miles in the last year. The goal is to try to identify clusters of passengers that have similar characteristics for the purpose of targeting different segments for different types of mileage offers.

```{r toolbox-and-working-directory-setups}

# Library imports

library(readxl)
library(dplyr)
library(ggplot2)
library(cluster)

# Setting the seed

seed <- 425

```

```{r dataset}

# Importing, modifying, and checking the data set

data        <- read_excel("EastWestAirlines.xlsx", sheet = "data")

data        <- data[,-1]                # Dropping the unnecessary index column

data.scaled <- apply(data,              # Scaling the data between 0 and 1
                     MARGIN = 2,
                     FUN = function(X) (X - min(X))/diff(range(X)))

summary(data.scaled)                    # Checking the modified version

```

## Hierarchical Clustering

Applying hierarchical clustering with Euclidean distance and complete linkage:

```{r hc}
# Applying hierarchical clustering

dm_a            <- dist(data.scaled,                    
                        method      = "euclidean")  # Dissimilarity matrix (Euclidean distance)

hc.complete.a   <- hclust(dm_a,
                          method    = "complete")   # Hierarchical clustering using complete linkage

sil.widths.a    <- c()                              # Empty vector to store silhouette widths
                                                    # for different values of K

for (i in 2:10) {                                   # The for loop that generates cluster sets and
                                                    # calculates their respective silhouette widths
    
    clust               <- cutree(hc.complete.a, k = i)
    
    sil                 <- silhouette(clust, dm_a)
    
    sil.widths.a[i-1]   <- mean(sil[, c("sil_width")])    
    
}

results_a <- data.frame(k = 2:10, sil.widths.a)     # Storing the results in a data frame

results_a                                           # Viewing the results

```

As can be seen in the above table, K = `r results_a[which.max(results_a$sil.widths.a), 1]` yields the highest silhouette width, which is `r round(results_a[which.max(results_a$sil.widths.a), 2], 2)`, implying that `r results_a[which.max(results_a$sil.widths.a), 1]` is the appropriate number of clusters to be used.

The next step is to compare the cluster centroids to characterize the different clusters and try to give each cluster a label. To check the final state of the clusters, the clustering with the best k value can be performed as follows:

```{r hcbesta}
# Performing the best clustering

hc.best.a     <- cutree(hc.complete.a, 
                        k = results_a[which.max(results_a$sil.widths.a), 1])

table(hc.best.a)

```

The above results indicate that two of the clusters dominates the data set, while the other one includes only 4 observations.


```{r centroid}
# Creating a table that displays the centroids of two clusters

data.w.clusters <- mutate(data, Cluster = hc.best.a)

first.cluster   <- data.w.clusters[data.w.clusters$Cluster == 1, ]

second.cluster  <- data.w.clusters[data.w.clusters$Cluster == 2, ]

third.cluster   <- data.w.clusters[data.w.clusters$Cluster == 3, ]

centroids       <- data.frame("1" = colMeans(first.cluster),
                              "2" = colMeans(second.cluster),
                              "3" = colMeans(third.cluster)     )

print(round(centroids, 2))

```

To obtain more insights about the clusters, centroids can be compared to the quartiles:

```{r quartile}

summary(data)

```

The three obtained clusters seem to be representing customer groups with different activity rates.

The third cluster have extremely high activity, when the averages of activity in the last year, bonus usage, and traveled miles of these customers are considered. Most of the features have higher average than the third quartile for this group. It can also be said that they are relatively new customers.

The first cluster consists of regular customers who travel occasionally. The second cluster have higher bonus rates and traveled distances in the last year, along with longer relationships with the company.

Based on these insights, the customers in the first, second, third clusters can be labelled as *usual customers*, *loyal customers*, and *super customers*, respectively.

To check the stability of the clusters, a random 5% of the data (200 observations) can be removed, and the analysis can be repeated with the modified data set.

```{r datadrop}
# Dropping 200 random rows from the data set

set.seed(seed)

drop        <- as.numeric(sample.int(3999, 200, replace = FALSE))

sliced.data <- data.scaled[-drop, ]

# Repeating the analysis

dm_b            <- dist(sliced.data,                    
                        method = "euclidean")       

hc.complete.b   <- hclust(dm_b,
                          method = "complete")      

sil.widths.b    <- c()                              

for (i in 2:10) {                                   
                                                    
    
    clust               <- cutree(hc.complete.b, k = i)
    
    sil                 <- silhouette(clust, dm_b)
    
    sil.widths.b[i-1]   <- mean(sil[, c("sil_width")])    
}

results_b <- data.frame(k = 2:10, sil.widths.b)       

results_b

```


```{r hcbestb}
# Checking the new clusters

hc.best.b <- cutree(hc.complete.b, 
                    k = results_b[which.max(results_b$sil.widths.b), 1])

table(hc.best.b)

```

Excluding 200 random observations did not cause dramatic changes in the clustering results. Considering the general non-robustness problem with clustering methods, it can be concluded that the current model is not fragile against the changes in the data set and gives useful insights.

## K-Means Clustering

Using K-Means algorithm with different number of clusters and determining the best number of clusters using the silhouette index:

```{r kmeans}
# Applying k-means clustering

dm_c            <- dist(data.scaled,
                        method = "euclidean")

sil.widths.c    <- c()

for (i in 2:10) {                                   
    
    clust               <- kmeans(data.scaled, 
                                  centers   = i, 
                                  nstart    = 50)
    
    sil                 <- silhouette(clust$cl, dm_c)
    
    sil.widths.c[i-1]   <- mean(sil[, c("sil_width")])    
}

results_c <- data.frame(k = 2:10, sil.widths.c)

results_c 

```

```{r kmeansbest}
# Checking the clusters obtained by using the best k value

km.best <- kmeans(data.scaled, 
                  centers   = results_c[which.max(results_c$sil.widths.c), 1], 
                  nstart    = 50)

table(km.best$cl)

```

Setting K = `r results_c[which.max(results_c$sil.widths.c), 1]` yields the highest silhouette width, which is `r round(results_c[which.max(results_c$sil.widths.c), 2], 2)`, implying that `r results_c[which.max(results_c$sil.widths.c), 1]` is the appropriate number of clusters to be used. It seems like K-Means clustering algorithm suggests that super customer is not an important insight. 

## Final Comments

Since major part of the customers fall into the first cluster in hierarchically clustered data, setting these customers as target audience is reasonable. Increasing the number of reward point campaigns might potentially convert these usual customers to loyal customers. Special campaigns and discounts for the loyal customers would also be useful to keep these customers loyal, or event convert them to super customers.

For the clusters obtained by K-Means algorithm, similar strategy can be used by picking the second cluster as the main target audience due to the higher number of observations fall into this set. The absence of super customers is not a decision criteria since their amount is negligible.
